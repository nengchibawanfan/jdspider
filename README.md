# jdspider


京东商品爬取和简单的数据分析

从京东的分类商品页获取数据。数据时ＡＪＡＸ动态加载，但未查到json格式数据，可能采用被加密，故采用Selenium使用浏览器进行爬取。
即添加中间件，使用Selenium通过chrome浏览器获取网页源代码，这时的网页源代码包含了所有的商品数据，再将源码返回，将其Scrapy的网页代码来源，继续对其进行解析，获取相应数据。将数据存储到文件和ＭｙＳＱＬ数据库


Scrapy框架介绍
Scrapy是基于Python实现的一种多线程快速爬取框架，使用了Twisted作为框架底层，其最大的做点是以事件作为驱动，适合做异步的任务，不会因某一任务未完成而产生阻塞导致别的进程也无法运行而浪费系统资源，如执行文件读取、数据库访问及网络请求时或Shell交互时，经常会发生阻塞，通过Twisted的事件驱动机制即可刷新因阻塞而降低程序效率的问题。所以该框架具有高性能、高定制性等特点，也是现在常用的爬虫框架之一，可以方便、快速地在网络中批量爬取自己需要的信息。


Scrapy框架中数据流动
（1）爬虫引擎从start_urls或start_requests中获取第一个链接，开始进行第一次的抓取。
（2）启动请求调度器，接受引擎发来的Requset请求并处理，进行入队操作，同时准备对下一次的请求的爬取。
（3）调度器将下一个需要爬取的网络请求返回给爬虫引擎。
（4）爬虫引擎将上一步获取的网络请求传送到下载器，通过下载中间件对相应的网页内容进行下载获取Response响应并交还给引擎。
（5）在下载器将相应的网页内容完全下载后，将下载的内容返回给爬虫引擎。
（6）爬虫引擎将下载器下载的内容通过spider中间件使用spider进行网页内容的解析，从而获取到需要的数据和链接。
（7）解析完毕后，将项目所需要的信息打包为Items对象返回，同时将网页中获取的下一个待爬取的链接作为Request请求对象并进行返回。
（8）网络引擎收到spider的返回对象后进行最后的处理。如果是Items对象则将其传到pipline中对数据做进一步的处理，如清洗、存储等；如果是Requests对象则将其传送至调试器，以计划下一次的爬取。
（9）检测是否完成所有的网络请求，如果已完全返回结束爬取，如果未完成，继续重复1-8





